# UVA Presentation — AI Image & Video Generation

Resources accompanying the guest lecture on generative AI tools, workflows, and creative practice.

**Slides:** [docs.google.com — UVA Presentation](https://docs.google.com/presentation/d/1qiZuJ-yLSgywyz4tdUbhd3LlUXqKL3mdnHRVgDd-s4E/edit?usp=sharing)

---

## What's in this repo

| File | Description |
|------|-------------|
| `vocabulary` | Definitions for AI/diffusion/ComfyUI terms covered in the lecture |
| `AI_best_practices` | Short list of practical guidelines for working with generative AI tools |
| `reference_links.html` | Categorized links to every tool, model, platform, and resource mentioned — open in a browser |
| `link_to_presentation` | Direct link to the Google Slides deck |

---

## ComfyUI + SD1.5 Local Setup

The hands-on demo uses a separate companion repo:

**[github.com/saloime/sd15_touch](https://github.com/saloime/sd15_touch)**

That repo includes everything needed to run Stable Diffusion 1.5 locally with ComfyUI, ControlNet, IP-Adapter, and TouchDesigner integration — including a one-click install script for both fresh installs and existing ComfyUI setups.

---

## Tools demoed

- **ComfyUI** — local node-based image generation
- **fal.ai / Weavy** — browser-based cloud inference
- **FLUX, Wan2.1, HunyuanVideo, LTX Video** — current open-source models
- **AI-Toolkit / SimpleTuner** — LoRA fine-tuning

---

*Chris Wright Evans — [chriswrightevans.photo](https://chriswrightevans.photo)*
